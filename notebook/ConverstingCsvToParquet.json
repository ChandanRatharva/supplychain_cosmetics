{
	"name": "ConverstingCsvToParquet",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0c86a9bc-d75b-4ccc-9647-373d76b45912"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8d10606e-c000-46dc-ac8b-ba398748c4d7/resourceGroups/dev_env_datahub_adf/providers/Microsoft.Synapse/workspaces/synapseanalyticscr/bigDataPools/SparkCluster",
				"name": "SparkCluster",
				"type": "Spark",
				"endpoint": "https://synapseanalyticscr.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.format(\"csv\") \\\n",
					"    .option(\"header\", \"true\") \\\n",
					"    .option(\"inferSchema\", \"true\") \\\n",
					"    .load(\"abfss://landing-zone@sadevenvc.dfs.core.windows.ne/staging/sqlServer/tempcsv/DimCustomer.csv\")\n",
					"\n",
					"df.show()\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from functools import reduce\n",
					"from pyspark.sql.functions import col\n",
					"\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"# Root paths\n",
					"csv_root     = \"abfss://sadevenvc@landing-zone.dfs.core.windows.net/staging/sqlServer/tempcsv\"\n",
					"parquet_root = \"abfss://sadevenvc@landing-zone.dfs.core.windows.net/staging/sqlServer/rawtables\"\n",
					"\n",
					"# Table list\n",
					"tables = [\n",
					"    \"DimCustomer\",\n",
					"    \"DimProduct\"\n",
					"]\n",
					"\n",
					"# 3) Loop over each table: read CSV ‚Üí append to Parquet\n",
					"for table in tables:\n",
					"    csv_path     = f\"{csv_root}/{table}.csv\"\n",
					"    parquet_path = f\"{parquet_root}/{table}\"\n",
					"    \n",
					"    print(f\"\\n‚ñ∂Ô∏è Processing table `{table}`\")\n",
					"    \n",
					"    # 3a) Read the incremental CSV\n",
					"    df = (\n",
					"        spark.read\n",
					"             .option(\"header\", True)\n",
					"             .option(\"inferSchema\", True)\n",
					"             .option(\"mode\",\"DROPMALFORMED\")\n",
					"             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
					"             .option(\"delimiter\", \",\")\n",
					"             .option(\"ignoreLeadingWhiteSpace\",True)\n",
					"             .option(\"ignoreTrailingWhiteSpace\",True)\n",
					"             .csv(csv_path)\n",
					"    )\n",
					"        \n",
					"        # Remove rows where all columns are null or empty\n",
					"    non_empty_conditions = [col(c).isNotNull() & (col(c).cast(\"string\") != \"\") for c in df.columns]\n",
					"    df_filtered = df.filter(reduce(lambda x, y: x | y, non_empty_conditions))\n",
					" \n",
					"    # Additional filter for tbAddress: Exclude rows where AddressID is only commas (e.g., \",,\" or \" , , \")\n",
					"    if table == \"DimCustomer\":\n",
					"        df_filtered = df_filtered.filter(\n",
					"            ~col(\"CustomerID\").rlike(r\"^\\s*,+\\s*$\") & col(\"CustomerID\").isNotNull()\n",
					"        )\n",
					" \n",
					"    count = df_filtered.count()\n",
					"    print(f\"   ‚Ä¢ Filtered and retained {count:,} non-empty rows from {csv_path}\")\n",
					"\n",
					"    spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\")\n",
					"\n",
					"    # 3b) Append into Parquet folder for this table\n",
					"    (\n",
					"        df.write\n",
					"          .mode(\"Overwrite\")    # append new files alongside existing ones\n",
					"          .parquet(parquet_path)\n",
					"    )\n",
					"    print(f\"   ‚úÖ Appended {count:,} rows to {parquet_path}\")\n",
					"\n",
					"print(\"\\nüéâ All 11 CSVs have been appended to their respective Parquet folders.\")\t\n",
					"\t"
				],
				"execution_count": 18
			}
		]
	}
}